{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t$,\n",
    "    where $ \\hat{x}$ is the estimated statistic and $x_t$ is the\n",
    "    new observed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maffine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D\n",
       "inputs with optional additional channel dimension) as described in the paper\n",
       "`Batch Normalization: Accelerating Deep Network Training by Reducing\n",
       "Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\n",
       "\n",
       ".. math::\n",
       "\n",
       "    y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
       "\n",
       "The mean and standard-deviation are calculated per-dimension over\n",
       "the mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\n",
       "of size `C` (where `C` is the input size). By default, the elements of :math:`\\gamma` are set\n",
       "to 1 and the elements of :math:`\\beta` are set to 0. The standard-deviation is calculated\n",
       "via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.\n",
       "\n",
       "Also by default, during training this layer keeps running estimates of its\n",
       "computed mean and variance, which are then used for normalization during\n",
       "evaluation. The running estimates are kept with a default :attr:`momentum`\n",
       "of 0.1.\n",
       "\n",
       "If :attr:`track_running_stats` is set to ``False``, this layer then does not\n",
       "keep running estimates, and batch statistics are instead used during\n",
       "evaluation time as well.\n",
       "\n",
       ".. note::\n",
       "    This :attr:`momentum` argument is different from one used in optimizer\n",
       "    classes and the conventional notion of momentum. Mathematically, the\n",
       "    update rule for running statistics here is\n",
       "    :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n",
       "    where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n",
       "    new observed value.\n",
       "\n",
       "Because the Batch Normalization is done over the `C` dimension, computing statistics\n",
       "on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.\n",
       "\n",
       "Args:\n",
       "    num_features: :math:`C` from an expected input of size\n",
       "        :math:`(N, C, L)` or :math:`L` from input of size :math:`(N, L)`\n",
       "    eps: a value added to the denominator for numerical stability.\n",
       "        Default: 1e-5\n",
       "    momentum: the value used for the running_mean and running_var\n",
       "        computation. Can be set to ``None`` for cumulative moving average\n",
       "        (i.e. simple average). Default: 0.1\n",
       "    affine: a boolean value that when set to ``True``, this module has\n",
       "        learnable affine parameters. Default: ``True``\n",
       "    track_running_stats: a boolean value that when set to ``True``, this\n",
       "        module tracks the running mean and variance, and when set to ``False``,\n",
       "        this module does not track such statistics, and initializes statistics\n",
       "        buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n",
       "        When these buffers are ``None``, this module always uses batch statistics.\n",
       "        in both training and eval modes. Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
       "    - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # With Learnable Parameters\n",
       "    >>> m = nn.BatchNorm1d(100)\n",
       "    >>> # Without Learnable Parameters\n",
       "    >>> m = nn.BatchNorm1d(100, affine=False)\n",
       "    >>> input = torch.randn(20, 100)\n",
       "    >>> output = m(input)\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.BatchNorm1d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d(nn.Module):\n",
    "    def __init__(self,\n",
    "    num_features,\n",
    "    eps=1e-05,\n",
    "    momentum=0.1,\n",
    "    affine=True,\n",
    "#     track_running_stats=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.running_var = torch.ones(num_features) #internal mean and bias estimates for use in inference\n",
    "        self.running_mean = torch.zeros(num_features)\n",
    "        self.mom = momentum\n",
    "        if affine:\n",
    "            self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training: #Normalize by batch mean/var, and update mean/var estimates\n",
    "            mean = torch.mean(x, 0, keepdim = True)\n",
    "            x = x - mean\n",
    "            var = torch.var(x, 0, keepdim = True, unbiased = False)\n",
    "            x = x/torch.sqrt(var + self.eps)\n",
    "            self.update_estimates(mean, var)\n",
    "        else: #Normalize by mean/var estimates from training\n",
    "            x = x - self.running_mean\n",
    "            x = x/torch.sqrt(self.running_var + self.eps)\n",
    "        if self.affine: #Apply learnable scale/offset\n",
    "            l = len(x.shape) #number of dimensions\n",
    "            trail_shape = (l - 2) * (1,) #ones of trailing dimensions after features\n",
    "            gamma = self.gamma.view((1,-1) + trail_shape) #view so that we multiply/add over features\n",
    "            beta = self.beta.view((1,-1) + trail_shape)\n",
    "            x = gamma*x + beta\n",
    "        return x\n",
    "\n",
    "    def update_estimates(self, mean, var):\n",
    "        self.running_mean = (1-self.mom)*self.running_mean + self.mom * mean.flatten()\n",
    "        self.running_var = (1-self.mom)*self.running_var + self.mom * var.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BatchNorm1d(3, affine=True)\n",
    "bnt = nn.BatchNorm1d(3, affine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8313,  0.7915,  1.1609],\n",
      "        [-1.6295, -0.5567, -0.6334],\n",
      "        [-0.0099, -1.3494,  0.7636],\n",
      "        [ 0.8081,  1.1146, -1.2912]], grad_fn=<NativeBatchNormBackward>)\n",
      "tensor([[ 0.8313,  0.7915,  1.1609],\n",
      "        [-1.6295, -0.5567, -0.6334],\n",
      "        [-0.0099, -1.3494,  0.7636],\n",
      "        [ 0.8081,  1.1146, -1.2912]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for f in [bnt, bn]:\n",
    "    o = f(data)\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1123, 0.1677, 0.1336])\n",
      "tensor([0.7472, 0.7675, 0.7440])\n",
      "tensor([0.1123, 0.1677, 0.1336])\n",
      "tensor([0.7427, 0.7578, 0.7403])\n"
     ]
    }
   ],
   "source": [
    "for f in [bnt, bn]:\n",
    "    print(f.running_mean)\n",
    "    print(f.running_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight difference in running_var. Maybe pytorch implementation updates std instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5656,  0.8098,  0.6908],\n",
      "        [-0.0738,  0.3077,  0.2669],\n",
      "        [ 0.3470,  0.0125,  0.5969],\n",
      "        [ 0.5596,  0.9301,  0.1114]], grad_fn=<NativeBatchNormBackward>)\n",
      "tensor([[ 0.5673,  0.8149,  0.6925],\n",
      "        [-0.0740,  0.3096,  0.2675],\n",
      "        [ 0.3481,  0.0126,  0.5984],\n",
      "        [ 0.5613,  0.9360,  0.1117]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for f in [bnt, bn]:\n",
    "    f.eval()\n",
    "    o = f(data)\n",
    "    print(o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
