{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\begin{array}{ll} \\\\\n",
    "        i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "        f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "        o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "        c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "        h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "    \\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(4,4)\n",
    "d,e,f,g = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.split(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_param(shape, extent):\n",
    "    return nn.Parameter(torch.FloatTensor(*shape).uniform_(*extent))\n",
    "\n",
    "def MVb(Mat, vecs):\n",
    "    '''Matrix multiply Mat by batched vectors vecs'''\n",
    "    vecs = vecs.unsqueeze(-1)\n",
    "    out = Mat @ vecs\n",
    "    out = out.squeeze(-1)\n",
    "    return out\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,\n",
    "                 #num_layers = 1, bias = True,\n",
    "                 #batch_first = False, dropout = 0,\n",
    "                 #bidirectional = False\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        #range of weight values for initialization:\n",
    "        k = 1/hidden_size\n",
    "        extent = (-math.sqrt(k), math.sqrt(k))\n",
    "        #shapes of weights and biases:\n",
    "        in_hid_shape = (4*hidden_size, input_size)\n",
    "        hid_hid_shape = (4*hidden_size, hidden_size)\n",
    "        bias_shape = (4*hidden_size,)\n",
    "        #Create initialized weights and biases:\n",
    "        self.weight_ih_l0 = get_uniform_param(in_hid_shape, extent)\n",
    "        self.weight_hh_l0 = get_uniform_param(hid_hid_shape, extent)\n",
    "        self.bias_ih_l0 = get_uniform_param(bias_shape, extent)\n",
    "        self.bias_hh_l0 = get_uniform_param(bias_shape, extent)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def single_forward(self, x, hc):\n",
    "        '''\n",
    "        Compute h and c given the current input x and previous hc\n",
    "        (where x is a single element of the sequence)\n",
    "        '''\n",
    "        h,c = hc\n",
    "        wis = self.weight_ih_l0.split(self.hidden_size) #split into 4 sets of weights\n",
    "        whs = self.weight_hh_l0.split(self.hidden_size)\n",
    "        bis = self.bias_ih_l0.split(self.hidden_size)\n",
    "        bhs = self.bias_hh_l0.split(self.hidden_size)\n",
    "        #weighted activations. for is messy but non-trivial to vectorize\n",
    "        As = [MVb(wis[i], x) + bis[i] + MVb(whs[i], h) + bhs[i] for i in range(4)]\n",
    "        g = self.tanh(As[0])\n",
    "        i,f,o = [self.sigmoid(a) for a in As[1:]] #i,f, and o\n",
    "        c = f * c + i * g\n",
    "        h = o * self.tanh(c)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, x, hc = None):\n",
    "        '''\n",
    "        Operate over sequence x of dim: seq x batch x features\n",
    "        '''\n",
    "        if hc is None: #initialize with hidden of all zeros:\n",
    "            h_shape = (1, x.shape[1], self.hidden_size)\n",
    "            hc = (torch.zeros(h_shape), torch.zeros(h_shape))\n",
    "        #allocate seq_len x batch x hidden for output:\n",
    "        outputs = torch.empty(x.shape[:2] + (self.hidden_size,))\n",
    "        #Iterate over input sequence, saving hidden state\n",
    "        for i, xi in enumerate(x):\n",
    "            hc = self.single_forward(xi, hc)\n",
    "            outputs[i] = hc[0] #h\n",
    "        return outputs, hc\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3147,  0.2199, -0.0194],\n",
      "         [-0.4538, -0.1486, -0.1851]],\n",
      "\n",
      "        [[ 0.5117,  0.3117, -0.0852],\n",
      "         [-0.0708, -0.1156,  0.0874]],\n",
      "\n",
      "        [[ 0.6136,  0.2184,  0.0378],\n",
      "         [ 0.1698, -0.1697,  0.1435]],\n",
      "\n",
      "        [[ 0.6082,  0.3992, -0.1496],\n",
      "         [ 0.0355, -0.0972,  0.0979]],\n",
      "\n",
      "        [[ 0.4133,  0.1604,  0.0671],\n",
      "         [ 0.1286, -0.1735,  0.2223]]], grad_fn=<CopySlices>)\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# lstm = nn.LSTM(4,3)\n",
    "my_lstm = LSTM(4,3)\n",
    "\n",
    "inputs = torch.randn(5,2,4)\n",
    "\n",
    "hidden = (torch.randn(1, 2, 3),\n",
    "          torch.randn(1, 2, 3))\n",
    "\n",
    "out, hidden = my_lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training Example\n",
    "\n",
    "From https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    # Tags are: DET - determiner; NN - noun; V - verb\n",
    "    # For example, the word \"The\" is a determiner\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(LSTM_class):\n",
    "    class LSTMTagger(nn.Module):\n",
    "\n",
    "        def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "            super(LSTMTagger, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "            # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "            # with dimensionality hidden_dim.\n",
    "            self.lstm = LSTM_class(embedding_dim, hidden_dim)\n",
    "\n",
    "            # The linear layer that maps from hidden state space to tag space\n",
    "            self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "            self.log_softmax = nn.LogSoftmax(1)\n",
    "\n",
    "        def forward(self, sentence):\n",
    "            embeds = self.word_embeddings(sentence)\n",
    "            lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "            tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "            tag_scores = self.log_softmax(tag_space)\n",
    "            return tag_scores\n",
    "\n",
    "    model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    # See what the scores are before training\n",
    "    # Note that element i,j of the output is the score for tag j for word i.\n",
    "    # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        print(tag_scores)\n",
    "\n",
    "    for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        for sentence, tags in training_data:\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # See what the scores are after training\n",
    "    with torch.no_grad():\n",
    "        inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "\n",
    "        # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "        # for word i. The predicted tag is the maximum scoring tag.\n",
    "        # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "        # since 0 is index of the maximum value of row 1,\n",
    "        # 1 is the index of maximum value of row 2, etc.\n",
    "        # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "        print(tag_scores)\n",
    "        print(tag_scores.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3887, -0.8282, -1.1591],\n",
      "        [-1.4371, -0.8561, -1.0860],\n",
      "        [-1.4444, -0.7989, -1.1574],\n",
      "        [-1.3369, -0.8006, -1.2439],\n",
      "        [-1.4216, -0.8232, -1.1406]])\n",
      "tensor([[-0.0429, -3.7287, -4.0211],\n",
      "        [-4.0891, -0.0540, -3.3296],\n",
      "        [-2.1677, -2.5137, -0.2174],\n",
      "        [-0.0175, -4.6791, -4.8214],\n",
      "        [-4.5135, -0.0223, -4.4985]])\n",
      "tensor([0, 1, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2070, -1.4539, -0.7609],\n",
      "        [-1.2626, -1.4106, -0.7484],\n",
      "        [-1.2714, -1.3578, -0.7715],\n",
      "        [-1.2227, -1.3853, -0.7868],\n",
      "        [-1.2283, -1.4457, -0.7516]])\n",
      "tensor([[-0.0377, -3.3528, -6.2028],\n",
      "        [-2.8416, -0.0918, -3.5260],\n",
      "        [-3.9234, -3.9229, -0.0404],\n",
      "        [-0.0428, -3.4167, -4.6988],\n",
      "        [-5.0078, -0.0087, -6.2402]])\n",
      "tensor([0, 1, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets reasonable looking answer, but rather slow, and more testing needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
