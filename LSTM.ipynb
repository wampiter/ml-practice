{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "    \\begin{array}{ll} \\\\\n",
    "        i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "        f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "        o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "        c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "        h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "    \\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_param(shape, extent):\n",
    "    return nn.Parameter(torch.FloatTensor(*shape).uniform_(*extent))\n",
    "\n",
    "def MVb(Mat, vecs):\n",
    "    '''Matrix multiply Mat by batched vectors vecs'''\n",
    "    vecs = vecs.unsqueeze(-1)\n",
    "    out = Mat @ vecs\n",
    "    out = out.squeeze(-1)\n",
    "    return out\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,\n",
    "                 #num_layers = 1, bias = True,\n",
    "                 #batch_first = False, dropout = 0,\n",
    "                 #bidirectional = False\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        #range of weight values for initialization:\n",
    "        k = 1/hidden_size\n",
    "        extent = (-math.sqrt(k), math.sqrt(k))\n",
    "        #shapes of weights and biases:\n",
    "        in_hid_shape = (4*hidden_size, input_size)\n",
    "        hid_hid_shape = (4*hidden_size, hidden_size)\n",
    "        bias_shape = (4*hidden_size,)\n",
    "        #Create initialized weights and biases:\n",
    "        self.weight_ih_l0 = get_uniform_param(in_hid_shape, extent)\n",
    "        self.weight_hh_l0 = get_uniform_param(hid_hid_shape, extent)\n",
    "        self.bias_ih_l0 = get_uniform_param(bias_shape, extent)\n",
    "        self.bias_hh_l0 = get_uniform_param(bias_shape, extent)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.tanh2 = nn.Tanh()\n",
    "\n",
    "    def single_forward(self, x, hc):\n",
    "        '''\n",
    "        Compute h and c given the current input x and previous hc\n",
    "        (where x is a single element of the sequence)\n",
    "        '''\n",
    "        h,c = hc\n",
    "        a = MVb(self.weight_ih_l0, x) + self.bias_ih_l0 #weighted activations\n",
    "        a = a + MVb(self.weight_hh_l0, h) + self.bias_hh_l0\n",
    "        a[:self.hidden_size] = self.tanh(a[:self.hidden_size]) #g\n",
    "        a[self.hidden_size:] = self.sigmoid(a[self.hidden_size:]) #i,f, and o\n",
    "        g,i,f,o = a.split(self.hidden_size, -1)\n",
    "        c = f * c + i * g\n",
    "        h = o * self.tanh2(c)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, x, hc = None):\n",
    "        '''\n",
    "        Operate over sequence x of dim: seq x batch x features\n",
    "        '''\n",
    "        if hc is None: #initialize with hidden of all zeros:\n",
    "            h_shape = (1, x.shape[1], self.hidden_size)\n",
    "            hc = (torch.zeros(h_shape), torch.zeros(h_shape))\n",
    "        #allocate seq_len x batch x hidden for output:\n",
    "        outputs = torch.empty(x.shape[:2] + (self.hidden_size,))\n",
    "        #Iterate over input sequence, saving hidden state\n",
    "        for i, xi in enumerate(x):\n",
    "            hc = self.single_forward(xi, hc)\n",
    "            outputs[i] = hc[0] #h\n",
    "        return outputs, hc\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.5651e-01,  5.7178e-01, -5.3439e-01],\n",
      "         [-1.2970e-02, -6.0097e-02,  1.6398e-01]],\n",
      "\n",
      "        [[ 1.6066e-04, -9.1151e-02,  8.4208e-02],\n",
      "         [-4.4458e-03,  6.0881e-02,  3.7133e-02]],\n",
      "\n",
      "        [[-1.2920e-01, -1.8123e-01, -6.0872e-02],\n",
      "         [ 1.2895e-01, -7.4025e-02,  1.5713e-01]],\n",
      "\n",
      "        [[-6.1014e-02, -7.2531e-02,  4.3606e-02],\n",
      "         [-3.3204e-02, -3.9318e-03, -2.5229e-02]],\n",
      "\n",
      "        [[-1.4493e-01, -3.1395e-01,  7.1604e-02],\n",
      "         [-1.6903e-02,  2.8890e-02,  6.4808e-02]]], grad_fn=<CopySlices>)\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# lstm = nn.LSTM(4,3)\n",
    "my_lstm = LSTM(4,3)\n",
    "\n",
    "inputs = torch.randn(5,2,4)\n",
    "\n",
    "hidden = (torch.randn(1, 2, 3),\n",
    "          torch.randn(1, 2, 3))\n",
    "\n",
    "out, hidden = my_lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training Example\n",
    "\n",
    "From https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    # Tags are: DET - determiner; NN - noun; V - verb\n",
    "    # For example, the word \"The\" is a determiner\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(LSTM_class):\n",
    "    class LSTMTagger(nn.Module):\n",
    "\n",
    "        def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "            super(LSTMTagger, self).__init__()\n",
    "            self.hidden_dim = hidden_dim\n",
    "\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "            # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "            # with dimensionality hidden_dim.\n",
    "            self.lstm = LSTM_class(embedding_dim, hidden_dim)\n",
    "\n",
    "            # The linear layer that maps from hidden state space to tag space\n",
    "            self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "            self.log_softmax = nn.LogSoftmax(1)\n",
    "\n",
    "        def forward(self, sentence):\n",
    "            embeds = self.word_embeddings(sentence)\n",
    "            lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "            tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "            tag_scores = self.log_softmax(tag_space)\n",
    "            return tag_scores\n",
    "\n",
    "    model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    # See what the scores are before training\n",
    "    # Note that element i,j of the output is the score for tag j for word i.\n",
    "    # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        print(tag_scores)\n",
    "\n",
    "    for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "        for sentence, tags in training_data:\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # See what the scores are after training\n",
    "    with torch.no_grad():\n",
    "        inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "\n",
    "        # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "        # for word i. The predicted tag is the maximum scoring tag.\n",
    "        # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "        # since 0 is index of the maximum value of row 1,\n",
    "        # 1 is the index of maximum value of row 2, etc.\n",
    "        # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "        print(tag_scores)\n",
    "        print(tag_scores.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9517, -1.3514, -1.0355],\n",
      "        [-0.9099, -1.3568, -1.0790],\n",
      "        [-0.9057, -1.3901, -1.0594],\n",
      "        [-0.9018, -1.3869, -1.0662],\n",
      "        [-0.8958, -1.3902, -1.0709]])\n",
      "tensor([[-0.1139, -2.8609, -2.9873],\n",
      "        [-1.7924, -0.1944, -4.5919],\n",
      "        [-2.0898, -5.1299, -0.1388],\n",
      "        [-0.0945, -2.8945, -3.3563],\n",
      "        [-3.5632, -0.0304, -6.4241]])\n",
      "tensor([0, 1, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8505, -1.2281, -1.2732],\n",
      "        [-0.8412, -1.2122, -1.3046],\n",
      "        [-0.8751, -1.2147, -1.2505],\n",
      "        [-0.8707, -1.1938, -1.2792],\n",
      "        [-0.9126, -1.2212, -1.1918]])\n",
      "tensor([[-3.6089e-02, -4.6551e+00, -3.6523e+00],\n",
      "        [-5.3321e+00, -1.4396e-02, -4.6608e+00],\n",
      "        [-3.9737e+00, -5.5045e+00, -2.3138e-02],\n",
      "        [-6.5820e-03, -6.0480e+00, -5.4732e+00],\n",
      "        [-6.7747e+00, -1.5989e-03, -7.6947e+00]])\n",
      "tensor([0, 1, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the right answer, but somehow is more confident than pytorch implementation? (also slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
